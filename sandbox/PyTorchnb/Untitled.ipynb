{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(1992)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 1) (2947, 128, 9) (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, testX, testy = utils.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = torch.tensor(trainX.transpose(0,2,1), dtype=torch.float).cuda()\n",
    "trainy = torch.tensor(trainy, dtype=torch.float).cuda()\n",
    "testX = torch.tensor(testX.transpose(0,2,1), dtype=torch.float).cuda()\n",
    "testy = torch.tensor(testy, dtype=torch.float).cuda()\n",
    "testy_c = testy.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 9, 6)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose, epochs, batch_size = True, 500, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[2], trainX.shape[1], trainy.shape[1]\n",
    "n_timesteps, n_features, n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(size, kernel_size):\n",
    "    coords = []\n",
    "    vals = []\n",
    "    for k in range(size):\n",
    "        for j in range(kernel_size):\n",
    "            i = k - j + 1\n",
    "            if k in range(size) and i in range(size) and j in range(kernel_size):\n",
    "                coords.append([k, i, j])\n",
    "                vals.append(1)\n",
    "\n",
    "    i = torch.LongTensor(coords)\n",
    "    v = torch.FloatTensor(vals)\n",
    "    return torch.sparse.FloatTensor(i.t(), v, torch.Size([size, size, kernel_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply(func, M, s):\n",
    "#     tList = [func(m, s) for m in torch.unbind(M) ]\n",
    "#     res = torch.stack(tList, dim=0)\n",
    "\n",
    "#     return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def toeplitz(kernel, input_size):\n",
    "#     half = int(np.ceil(len(kernel)/2))\n",
    "#     fst = torch.flip(kernel[0:half], [0])\n",
    "#     sec = kernel[half-1:]\n",
    "#     t = linalg.toeplitz(np.pad(sec, (0, input_size - len(sec))), np.pad(fst, (0, input_size - len(fst))))\n",
    "#     return torch.tensor(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dummy(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size):\n",
    "#         super(Dummy, self).__init__()\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.weights = torch.eye(self.num_features).expand((self.size, self.num_features, self.num_features)).float()#.cuda()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         print(x.shape, self.weights.shape)\n",
    "#         # X : (output_size, batch_size, num_features)\n",
    "#         return torch.bmm(x, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 10\n",
    "F = 3\n",
    "t = 5\n",
    "OUT = 6\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(batch, F, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(OUT, k).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = build_tensor(t, k).to_dense().expand(F, t, t, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5, 3])"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6])"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.matmul(T, w)#.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5, 6])"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7851, 0.2857, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0254, 0.7851, 0.2857, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0254, 0.7851, 0.2857, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0254, 0.7851, 0.2857],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0254, 0.7851]])"
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9566, 0.1663, 0.1594, 0.1825, 0.0993])"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7985, 0.2004, 0.1815, 0.1757, 0.0826])"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(W[0, :, :, 0],  X[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 6])"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensordot(X, W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4117, 1.2699, 1.5082, 1.5161, 0.7790])"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensordot(X, W)[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 3])"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3, 128, 128])"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensordot(w, T).expand(9, 64, 128, 128).permute(0,2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 128, 64, 128])"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 9, 128])"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 64, 128])"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensordot(X, W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 64, 128])"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensordot(X, W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Conv1d(9, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.conv.Conv1d"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 3])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv1d(nn.Module):\n",
    "    def __init__(self, size, in_channels, out_channels, kernel_size):\n",
    "        super(CustomConv1d, self).__init__()\n",
    "        self.size = size\n",
    "        self.num_features = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        y = 1.0 / np.sqrt(in_channels)\n",
    "        sampler = torch.distributions.Uniform(low=-y, high=y) \n",
    "        A = sampler.sample((self.out_channels, self.num_features, self.kernel_size)).cuda()\n",
    "        self.T = build_tensor(self.size, self.kernel_size).to_dense().expand(self.num_features, size, size, kernel_size).permute(0, 3, 1, 2).cuda()\n",
    "#         self.weights = nn.Parameter(apply(toeplitz, A, self.size).expand(self.num_features, self.out_channels, self.size, self.size).permute(0,2,1,3).clone().cuda())\n",
    "        self.weights = nn.Parameter(A)\n",
    "        self.weights.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "#         print(\"T: \", self.T.shape)\n",
    "        W = torch.tensordot(self.weights, self.T).expand(self.num_features, self.out_channels, self.size, self.size).permute(0,2,1,3)\n",
    "#         W = torch.matmul(self.T, self.weights).expand(self.num_features, self.size, self.size, self.out_channels).view(self.num_features, self.size, self.out_channels, self.size)#permute(0,1,3,2)\n",
    "#         print(\"X: \", x.shape)\n",
    "#         print(\"W: \", W.shape)\n",
    "        x = torch.tensordot(x, W)\n",
    "        \n",
    "#         end.record()\n",
    "#         torch.cuda.synchronize()\n",
    "\n",
    "#         print(\"TENSORDOT: \", start.elapsed_time(end))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomConv1d(nn.Module):\n",
    "#     def __init__(self, size, in_channels, out_channels, kernel_size):\n",
    "#         super(CustomConv1d, self).__init__()\n",
    "#         self.size = size\n",
    "#         self.num_features = in_channels\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.out_channels = out_channels\n",
    "#         y = 1.0 / np.sqrt(in_channels)\n",
    "#         sampler = torch.distributions.Uniform(low=-y, high=y) \n",
    "#         A = sampler.sample((self.out_channels, self.kernel_size))\n",
    "\n",
    "#         self.weights = nn.Parameter(apply(toeplitz, A, self.size).expand(self.num_features, self.out_channels, self.size, self.size).clone().permute(0,2,1,3).cuda())\n",
    "#         self.weights.requires_grad = True\n",
    "#         print(\"CALL\")\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.tensordot(x, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv1d(n_timesteps, n_features, 64, 3)\n",
    "        self.relu1 = nn.Tanh()\n",
    "        self.conv2 = CustomConv1d(n_timesteps, 64, 64, 3)\n",
    "        self.relu2 = nn.Tanh()\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.maxp = nn.AvgPool1d(2)\n",
    "        self.flat = nn.Flatten(1,-1)\n",
    "        self.dense1 = nn.Linear(4096, 100)\n",
    "        self.relu3 = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(100, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         start = torch.cuda.Event(enable_timing=True)\n",
    "#         end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "#         start.record()\n",
    "        x = self.conv1(x)\n",
    "\n",
    "#         end.record()\n",
    "#         torch.cuda.synchronize()\n",
    "\n",
    "#         print(\"CUSTOM_LAYER: \", start.elapsed_time(end))\n",
    "\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.maxp(x)\n",
    "        x = self.flat(x)\n",
    "        \n",
    "#         start = torch.cuda.Event(enable_timing=True)\n",
    "#         end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "#         start.record()\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "#         end.record()\n",
    "#         torch.cuda.synchronize()\n",
    "\n",
    "#         print(\"DENSE: \", start.elapsed_time(end))\n",
    "\n",
    "        x = self.relu3(x)\n",
    "        x = self.dense2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'batch_size': batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Epoch: 1\tLoss: 1.8585984706878662\tAcc: 0.11887921653971709\n",
      "Epoch: 2\tLoss: 2.5444517135620117\tAcc: 0.18920021762785635\n",
      "Epoch: 3\tLoss: 2.4243907928466797\tAcc: 0.18838411316648532\n",
      "Epoch: 4\tLoss: 2.2250819206237793\tAcc: 0.1882480957562568\n",
      "Epoch: 5\tLoss: 2.067070484161377\tAcc: 0.20389009793253537\n",
      "Epoch: 6\tLoss: 1.9329267740249634\tAcc: 0.16648531011969533\n",
      "Epoch: 7\tLoss: 1.7174493074417114\tAcc: 0.16376496191512513\n",
      "Epoch: 8\tLoss: 1.9999536275863647\tAcc: 0.12023939064200218\n",
      "Epoch: 9\tLoss: 1.7145600318908691\tAcc: 0.25068008705114253\n",
      "Epoch: 10\tLoss: 1.7372475862503052\tAcc: 0.3041349292709467\n",
      "Epoch: 11\tLoss: 1.6658921241760254\tAcc: 0.23993471164309033\n",
      "Epoch: 12\tLoss: 1.6845120191574097\tAcc: 0.22320457018498369\n",
      "Epoch: 13\tLoss: 1.7191433906555176\tAcc: 0.20837867247007616\n",
      "Epoch: 14\tLoss: 1.707417607307434\tAcc: 0.2146354733405876\n",
      "Epoch: 15\tLoss: 1.6619631052017212\tAcc: 0.2310935799782372\n",
      "Epoch: 16\tLoss: 1.6326324939727783\tAcc: 0.3216811751904244\n",
      "Epoch: 17\tLoss: 1.6402760744094849\tAcc: 0.34289989118607184\n",
      "Epoch: 18\tLoss: 1.5685726404190063\tAcc: 0.35459738846572364\n",
      "Epoch: 19\tLoss: 1.5509804487228394\tAcc: 0.3290261153427639\n",
      "Epoch: 20\tLoss: 1.548309564590454\tAcc: 0.3501088139281828\n",
      "Epoch: 21\tLoss: 1.5394936800003052\tAcc: 0.3554134929270947\n",
      "Epoch: 22\tLoss: 1.513776421546936\tAcc: 0.3705114254624592\n",
      "Epoch: 23\tLoss: 1.4833472967147827\tAcc: 0.40968443960826983\n",
      "Epoch: 24\tLoss: 1.457709550857544\tAcc: 0.4455930359085963\n",
      "Epoch: 25\tLoss: 1.44385826587677\tAcc: 0.4440968443960827\n",
      "Epoch: 26\tLoss: 1.4319802522659302\tAcc: 0.4470892274211099\n",
      "Epoch: 27\tLoss: 1.4149425029754639\tAcc: 0.45239390642002175\n",
      "Epoch: 28\tLoss: 1.3967316150665283\tAcc: 0.4751088139281828\n",
      "Epoch: 29\tLoss: 1.3915292024612427\tAcc: 0.4921109902067465\n",
      "Epoch: 30\tLoss: 1.379500150680542\tAcc: 0.470620239390642\n",
      "Epoch: 31\tLoss: 1.3703440427780151\tAcc: 0.4734766050054407\n",
      "Epoch: 32\tLoss: 1.3560662269592285\tAcc: 0.48653427638737756\n",
      "Epoch: 33\tLoss: 1.3371038436889648\tAcc: 0.491974972796518\n",
      "Epoch: 34\tLoss: 1.3240257501602173\tAcc: 0.46082698585418935\n",
      "Epoch: 35\tLoss: 1.3069709539413452\tAcc: 0.45661044613710555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-658-579ee3a1dba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainy_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainy_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}\\tLoss: {}\\tAcc: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Zero gradients, perform a backward pass, and update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainy_c = trainy.max(dim=1)[1]\n",
    "print(len(list(model.parameters())))\n",
    "for t in range(epochs):\n",
    "    y_pred = model(trainX)\n",
    "    criterion = nn.NLLLoss()\n",
    "    loss = criterion(torch.log(y_pred), trainy_c)\n",
    "    acc_train = float((trainy_c.long().eq(torch.max(y_pred, 1)[1].long())).sum())/(float(trainy_c.shape[0]))\n",
    "    print(\"Epoch: {}\\tLoss: {}\\tAcc: {}\".format(t+1, loss, acc_train))\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model(testX)\n",
    "acc_test = float((testy_c.long().eq(torch.max(y_pred_test, 1)[1].long())).sum())/(float(testy_c.shape[0]))\n",
    "print(\"TestAcc: {}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.view((128,9,2947))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([np.array(testX[0].cpu())]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = testy.max(dim=1)[1][0].float()\n",
    "y_pred = model.forward(sample).max(dim=1)[1].float()\n",
    "criterion = nn.NLLLoss()\n",
    "loss = criterion(torch.log(y_pred), y_true)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy_c.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []\n",
    "for i in range(trainX.shape[0]):\n",
    "    T.append(trainX[i].T)\n",
    "T = np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
